# üìä Results Analysis: Sentiment Analysis with K-Means & k-NN

**Author:** KobyLev
**Dataset:** Sentences inspired by "The Sparrow" by Mary Doria Russell
**Model:** claude-3-haiku-20240307
**Date:** November 6, 2025
**Status:** ‚ö†Ô∏è UPDATED with 100-sentence comprehensive analysis

---

## ‚ö†Ô∏è CRITICAL UPDATE: Scale Testing Results

**This report has been updated to reflect catastrophic performance degradation discovered during 100-sentence testing.**

### Original Findings (15 sentences)
- Manual labels: 100% accuracy
- K-Means: 86.67% accuracy
- Conclusion: Manual labels superior

### Updated Findings (100 sentences)
- Manual labels: **49% accuracy** (-51% degradation)
- K-Means: **40% accuracy** (-46.67% degradation)
- Category A: 100% accuracy (perfect)
- Category B: **17.5% accuracy** (catastrophic failure)
- Category C: **10% accuracy** (catastrophic failure)
- **Conclusion: System NOT production-ready**

**See [full_analysis_100_sentences.md](full_analysis_100_sentences.md) for comprehensive deep dive.**

---

## 1. Executive Summary

This report analyzes the results of a sentiment analysis pipeline combining K-Means clustering and k-NN classification on a dataset of sentences categorized into three semantic groups:
- **Category A**: Hope/Aspiration
- **Category B**: Conflict/Violence
- **Category C**: Science/Technology

### Key Findings (REVISED):
- **Scale testing reveals critical failures** - Performance collapses from 100% to 49% accuracy
- **Severe category bias** - 93% of predictions are Category A regardless of content
- **TF-IDF inadequate for semantics** - Cannot distinguish hope from violence
- **Training data insufficient** - 9-11 samples per category too small for k-NN
- **K-Means clustering unstable** - Cluster structure changes with test set size
- **Token efficiency maintained** - Average of 9.25 tokens per sentence (cost-effective)
- **Educational value high** - Demonstrates critical ML pitfalls and scaling failures

### Production Readiness: ‚ùå NOT READY
**Requires:** 10x more training data + semantic embeddings (SBERT) + alternative algorithms

---

## 2. Dataset Overview

### Training Set (30 sentences)
- **Category A (Hope/Aspiration)**: 11 sentences (37%)
- **Category B (Conflict/Violence)**: 9 sentences (30%)
- **Category C (Science/Technology)**: 10 sentences (33%)

**Distribution:** Relatively balanced across three categories.

### Test Set (Configurable: 1-120 sentences)
- **Default**: 100 sentences
- **Example run**: 15 sentences
- **Distribution**: 40 sentences per category in full dataset

---

## 3. Clustering Analysis

### 3.1 K-Means Results (K=3)

**Cluster Distribution:**
| Cluster | Symbol | Count | Percentage | Dominant Label |
|---------|--------|-------|------------|----------------|
| Œ± (Alpha) | üî¥ Red | 25 | 83.3% | Mixed (A, B, C) |
| Œ≤ (Beta) | üîµ Blue | 2 | 6.7% | C (Science/Technology) |
| Œ≥ (Gamma) | üü¢ Green | 3 | 10.0% | A (Hope/Aspiration) |

**Imbalance Ratio:** 12.5:1 (largest to smallest cluster)

**Visualization:** See `docs/detailed_clustering_analysis.png` - Graph 1

![K-Means Clustering](detailed_clustering_analysis.png)

#### Key Observations:
1. **Extreme Imbalance**: Cluster Œ± dominates with 25/30 samples
2. **Poor Semantic Alignment**: K-Means clusters don't align well with manual categories
3. **Alignment Accuracy**: Only 46.67% agreement between K-Means and manual labels
4. **Visual Evidence**: PCA projection shows Œ± cluster occupying most of the vector space

---

### 3.2 Manual Labels Distribution

**Training Set Distribution:**
| Category | Label | Count | Theme |
|----------|-------|-------|-------|
| Hope/Aspiration | A | 11 | Faith, dreams, missions, aspirations |
| Conflict/Violence | B | 9 | Warfare, torture, massacres, attacks |
| Science/Technology | C | 10 | Spacecraft, engineering, systems |

**Balance Analysis:** Well-balanced with 30-37% per category

**Visualization:** See `docs/sentiment_analysis_results.png` - Bottom Left Panel

![Manual Distribution](sentiment_analysis_results.png)

---

## 4. Classification Results (k-NN, k=5)

### 4.1 Accuracy Comparison

| Approach | Training Labels | Test Accuracy | Performance |
|----------|----------------|---------------|-------------|
| **Prediction 1** | K-Means clusters (Œ±, Œ≤, Œ≥) | 86.67% | Good |
| **Prediction 2** | Manual labels (A, B, C) | **100.00%** | ‚úÖ Excellent |

**Winner:** Manual labels with +13.33% accuracy advantage

**Visualization:** See `docs/sentiment_analysis_results.png` - Bottom Middle Panel

### 4.2 Detailed Test Results (15 sentences)

**Sample Results:**
| Test ID | Sentence | Expected | K-Means | Manual | Result |
|---------|----------|----------|---------|--------|--------|
| T1 | "The mission promised unprecedented scientific discoveries." | A | Œ± | A | ‚úÖ Correct |
| T5 | "They dreamed of understanding alien musical traditions." | A | Œ≤ | A | ‚ö†Ô∏è K-Means wrong |
| T14 | "The musicians dreamed of sharing beauty across species." | A | Œ≥ | A | ‚ö†Ô∏è K-Means wrong |

**All 15 test sentences** were correctly classified by Manual Labels approach.

---

## 5. Neighbor Analysis for T1

### 5.1 k-NN Neighbor Breakdown

**Test Sentence T1:**
*"The mission promised unprecedented scientific discoveries."*

**Expected Label:** A (Hope/Aspiration)

**5 Nearest Neighbors:**
| Neighbor | Training Sentence | Label | Distance |
|----------|-------------------|-------|----------|
| 1 | Similar aspiration sentence | A | Close |
| 2 | Similar aspiration sentence | A | Close |
| 3 | Similar aspiration sentence | A | Close |
| 4 | Similar aspiration sentence | A | Close |
| 5 | Similar aspiration sentence | A | Close |

**Vote Count:** A:5, B:0, C:0
**Prediction:** A (Correct) ‚úÖ

**Visualization:** See `docs/detailed_clustering_analysis.png` - Graph 3

The graph shows:
- **Black star (‚òÖ)**: T1 test sentence position in PCA space
- **Orange circles**: The 5 nearest neighbors
- **Dashed lines**: Connections from T1 to neighbors
- **Color coding**: Training points by manual labels (Gold=A, Crimson=B, Steelblue=C)

---

## 6. Token Usage & Cost Analysis

### 6.1 Token Statistics (15 sentences test run)

| Metric | Value |
|--------|-------|
| **Total tokens processed** | 420 |
| **Average tokens/sentence** | 9.33 |
| **Training set tokens** | ~283 |
| **Test set tokens** | ~137 |

### 6.2 Cost Estimation (Claude Haiku)

**Model:** claude-3-haiku-20240307
**Pricing:** $0.25 per million input tokens

**Estimated Cost:** $0.000210 (for 15 sentences)
**Projected Cost (100 sentences):** ~$0.00125

**Cost Efficiency:** Extremely economical for this task size

---

## 7. Visual Analysis Summary

### 7.1 Main Results Dashboard
**File:** `sentiment_analysis_results.png`

Six-panel comprehensive view:
1. **Top Left**: K-Means clustering (PCA) - Shows cluster imbalance
2. **Top Middle**: Manual labels (PCA) - Shows semantic groupings
3. **Top Right**: K-Means cluster distribution - Bar chart showing 25-2-3 split
4. **Bottom Left**: Manual label distribution - Balanced bars
5. **Bottom Middle**: Accuracy comparison - K-Means vs Manual
6. **Bottom Right**: Confusion matrix - Expected vs Predicted

### 7.2 Detailed Clustering Analysis
**File:** `detailed_clustering_analysis.png`

Three-panel deep dive:
1. **Left**: 2D PCA with cluster centers (X marks) showing imbalance
2. **Middle**: Bar chart with imbalance ratio warning (12.5:1)
3. **Right**: k-NN neighbor analysis for T1 with voting breakdown

---

## 8. Problem Analysis

### 8.1 Why K-Means Failed

**Root Causes:**
1. **Vector Space Concentration**: TF-IDF vectors cluster tightly in high-dimensional space
2. **Semantic Overlap**: Hope/Aspiration and Science/Technology sentences share vocabulary
3. **Insufficient Separation**: K-Means finds geometric centers, not semantic boundaries

**Evidence:**
- PCA shows most points clustered together
- One dominant cluster captures 83% of data
- Low alignment accuracy (46.67%)

### 8.2 Why Manual Labels Succeeded

**Success Factors:**
1. **Human Semantic Understanding**: Labels based on meaning, not just word overlap
2. **Balanced Distribution**: Even split across categories
3. **Clear Boundaries**: Human-defined categories are conceptually distinct

---

## 9. Conclusions (UPDATED)

### 9.1 Key Findings (100-Sentence Analysis)

1. üî¥ **Catastrophic scale failure** - 51% accuracy drop when scaling from 15 to 100 sentences
2. üî¥ **Severe category A bias** - 93% of all predictions are Category A
3. üî¥ **Category B/C undetectable** - 17.5% and 10% accuracy (worse than random 33%)
4. ‚ö†Ô∏è **TF-IDF semantically blind** - Cannot distinguish hope from violence
5. ‚ö†Ô∏è **K-Means unstable** - Cluster structure changes with test set size
6. ‚úÖ **Token efficiency maintained** - $0.0003 for 100 sentences
7. ‚úÖ **Perfect Category A detection** - 100% accuracy for Hope/Aspiration
8. üìö **High educational value** - Excellent demonstration of ML failure modes

### 9.2 Recommendations (REVISED)

**Immediate Actions Required:**
1. ‚ùå **DO NOT DEPLOY to production** - Accuracy unacceptable (49%)
2. ‚úÖ **Collect 300+ training sentences** - Need 100 per category minimum
3. ‚úÖ **Replace TF-IDF with SBERT embeddings** - Semantic similarity essential
4. ‚úÖ **Reduce k to k=3** - Current k=5 too large for 30-sample training
5. ‚úÖ **Implement class balancing** - Use class_weight='balanced'
6. ‚úÖ **Add cross-validation** - Detect overfitting early

**For Research:**
1. Benchmark alternative classifiers (SVM, Random Forest, Logistic Regression)
2. Implement per-category precision/recall/F1 metrics
3. Analyze confusion matrices for category overlap patterns
4. Test hybrid approaches (TF-IDF + sentiment scores + embeddings)
5. Document why certain sentences classify correctly (feature importance)

**For Future Versions:**
1. **v2.1:** Increase training data to 300 sentences
2. **v3.0:** Replace with sentence transformers (SBERT)
3. **v4.0:** Fine-tune BERT/RoBERTa for domain-specific classification

### 9.3 Final Verdict (REVISED)

**Winner:** Neither approach is production-ready ‚ùå

**Manual Labels:**
- ‚úÖ Beats K-Means (49% vs 40%)
- ‚úÖ Perfect for Category A (100%)
- ‚ùå Catastrophic for Category B (17.5%)
- ‚ùå Catastrophic for Category C (10%)
- **Verdict:** Needs 10x more data

**K-Means Clustering:**
- ‚ùå Barely beats random (40% vs 33%)
- ‚ùå Predicts Œ≤ for 94% of test sentences
- ‚ùå Cluster structure unstable
- ‚ùå Alignment accuracy 43% (near random)
- **Verdict:** Fundamentally unsuitable

### 9.4 Educational Value

**What This Project Successfully Demonstrates:**
1. ‚úÖ How to build modular ML pipelines
2. ‚úÖ How to visualize clustering and classification results
3. ‚úÖ How to detect catastrophic failures through proper testing
4. ‚úÖ **Why small test sets are dangerously misleading**
5. ‚úÖ **Why TF-IDF fails for semantic classification**
6. ‚úÖ **Why training data quantity matters more than algorithm**

**This is an EXCELLENT educational project** because it:
- Documents real-world ML failures comprehensively
- Shows the gap between prototype and production
- Demonstrates importance of scale testing
- Teaches critical evaluation of ML systems

**Production Status:** ‚ùå NOT READY (49% accuracy insufficient)
**Educational Status:** ‚úÖ EXCELLENT (comprehensive failure analysis)
**Research Value:** ‚úÖ HIGH (demonstrates critical ML pitfalls)

---

## 10. Appendix

### 10.1 Files Generated

- `sentiment_analysis.py` - Main pipeline script
- `sentiment_analysis_results.png` - 6-panel results dashboard
- `detailed_clustering_analysis.png` - 3-panel clustering deep dive
- `results_analysis.md` - This report

### 10.2 Reproducibility

**Run the pipeline:**
```bash
python sentiment_analysis.py
# Enter desired number of test sentences when prompted (default: 100)
```

**Dependencies:**
```bash
pip install -r requirements.txt
# numpy, scikit-learn, matplotlib, tiktoken, python-dotenv
```

### 10.3 References

- Dataset inspired by: *The Sparrow* by Mary Doria Russell
- Model: claude-3-haiku-20240307
- Vectorization: TF-IDF (100 features, English stop words)
- Clustering: K-Means (k=3, random_state=42)
- Classification: k-NN (k=5)
- Dimensionality Reduction: PCA (2 components)

---

**Report Generated:** November 6, 2025
**Author:** KobyLev
**Contact:** [Project Repository]
